# Инструкция

Для начала Вам необходимо проверить, что следующие программы установлены, а в противном случае их установить:

* **Python 2.7** с установленными пакетами

  * **pip** (в версиях Python 2.7.9 и далее он уже установлен)
  
  * **setuptools** (в версиях Python 2.7.9 и далее он уже установлен)
  
  * **lxml**
  
  * **OpenSSL**
  
  * **scrapy** (его устанавливайте последним - он требует всех других пакетов)
  
* **Midnight Commander** версия 2.8.10 или раньше (в более поздпних версиях нет нужной опции) или любая другая программа, с помощью которой можно безопасно и надёжно обмениваться файлами между локальным компьютером и серверами в Интернете.: **FileZilla**, **CuteFTP** и т.д.

* **Putty** (для Windows, чтобы подключаться к серверу)

## Установка пакетов в Python 2.7

1. В командной строке дойти до папки "Python27\Scripts"  с помощью команды **"cd `<path to folder>`**.

2. Произвести установку с помощью команды **"pip install `<package name>`"**.

3. Чтобы проверить, что пакет установился, можно забить **"pip freeze"** и посмотреть список всех установленных пакетов.

## Использование Putty

1. Запустите **Putty**.

2. Введите "Host Name" - в нашем случае **"new_words@web-corpora.net"**.

3. Введите Ваш **супер-секретный пароль** (то, что вводится пароль никак не будет отражаться на экране) и нажмите Enter.

4. Вам должно высветиться что-то подобное **"username@someHostName:~$"**, это значит, что Вы подключились к серверу.

## Команды на сервере

Можно посмотреть список различных команд на сайте **http://putty.eves.ru/**. Если не найдете там, то скорее всего нужную команду можно отыскать где-ниюудь в интренете. Некоторые команды представлены ниже.

* **ls** - показывает, где Вы находитесь на сервере в данный момент (какие папки, архивы, файлы там находятся)

* **cd `<archive or folder name>`** - заходит в папку или архив

* **nano `<file name>`** - открывает файл

* **.** - путь к той папке, в которой Вы сейчас находитесь (облегчает работу при написании пути)

* **cd ../** - возвращает Вас на папку выше

* **rm `<file name>`** - удаляет файл 

## Подготовка краулеров

Сами краулеры лежат в папке **thai_scrapy** данного репозитория. Необходимо скачать **всю папку "thai_scrapy"** так, чтобы внутрення структура осталась такой же.

Как подстроить краулер под конкретный сайт:

1. Открыть **"thai_spyder"** (путь - "thai_scrapy\thai_scrapy\spiders") 

 1. В этом коде добавить **новый класс**, имеющий вид уже написанных классов. 

 2. В новом, созданном Вами классе, изменить название класса на **"`<name>`Spider"** 

 3. В строчке **"name = '`<name>`'"** изменить старое имя на новое (то, что вы присвоили до слова "Spider" в названии самого класс)

 4. В строчке **"allowed_domains = [u'`<website>`']"** заменить старую ссылку сайта на новую **(без "http://www.")**

 5. В строчке **"start_urls = [u'`<website>`',]"** заменить старую ссылку сайта на новую **(с "http://www." и, если необходимо, с чем-то после слеша)**
 
 6. В конце класса в функции "parse_items" d строчке **"paragraphs = hxs.xpath(u'`<xml>`')"** изменить старый xml-код параграфов на новый. Новый код Вы можете найти на сайте, с которого Вы собираетесь скачивать страницы. Необходимо найти тот код, который содержит в себе большие куски текста, которые Вам и нужны. В Google Chrome это удобно сделать: найти нужный кусок текста на страницы, щелкнуть на правую клавишу мыши и выбрать "Просмотреть код". Справа Вам высветиться код.

 7. В строчке **"title = hxs.xpath(u'`<xml>`')"** изменить старый xml-код названия на новый. Новый код Вы можете найти на сайте, с которого Вы собираетесь скачивать страницы. Необходимо найти тот код, который содержит в себе название текста, который Вы скачиваете. Также удобно сделать в Google Chrome.
 
2. Открыть **"pipelines"** (путь - "thai_scrapy\thai_scrapy")

 1. В этом коде в **"with codecs.open(u'`<website>`' + str(item[u'name'])"** ссылку старого сайта замените на ссылку нового (соблюдайте при это формата из изначального файла: все сэши, точки и т.д.)
 
 2. Если необходимо, измените жанр внутри **"`<genre>` `</genre>`"**

## Проверка краулеров

До того, как Вы закачаете краулеры на сервер и запустите их, стоит проверить работают ли они. Для этого:

1. Создайте в верхней папке "thai_scrapy" пустую папку, куда будут сохранятся скаченные страницы. Для удобства ее хорощо бы назвать ссылкой сайта, с которого Вы эти страницы и выкачивали.

2. В командной строке спуститься до верхней папки "thai_scrapy".

3. Написать **"scrapy crawl `<name>`"**. `<name>` - это то, что у Вас написано в строке **"name = '`<name>`'"** файла "thai_spyder.py".

4. Проверить, что в пустой папке, которую Вы создали в шаге 1, появляются файлы и что эти файлы правильного формата (есть какие-нибудь тексты, заключенные в `<text>` `</text>` и `<title>` `</title>`, и указаны правильная ссылка и правильный жанр внутри соответсвующих тэгов).

5. Удалить файлы из этой папки, но оставить.

## Закачивание краулеров на сервер

1. С помощью **Midnight commander** (версия 2.8.10 или раньше)

 1. При запуске программы в обоих окнах будут отображаться каталоги Вашего компьютера. 

 2. Для соединения с сервером нужно кликнуть либо на "Right", либо на "Left" (в зависимости от того, какое из окон Вы хотите, чтобы отображало сервер) и выбрать "Shell link..." (такой опции нет в версиях позже 4.8.10, поэтому стоит скачивать именно ее)

 3. Совершить соединение с помощью логина и пароля.

 4. Перенести нужные файлы (вся верхняя папка "thai_scrapy") с компьютера на сервер (на место уже существующей там верхней папки "thai_scrapy").

2. С помощью **FileZilla**

 1. Запустить программу. Слева будет отображен так называемый "Локальный сайт", справа "Удаленный сайт", к которому Вы еще не подключены. 

 2. Сверху Вам предлагается совершить **"Быстрое подключение"** - это Вы и должны сделать. В поле "Хост" необходимо вбить **"sftp://web-corpora.net"**, в поле "Имя пользователя" - **"username"**, в поле "Пароль" - ваш супер-секретный пароль, а в поле "Порт" - **"22"**. После этого нажать "Быстрое соединение". Справа должны появится каталоги сервера.
 
 3. Для переброса нужных файлов на сервер кликает **на верхнюю папку "thai_scrapy"** и перетащить ее в правое окно. Вас будут спрашивать, нормально ли перезаписать уже существующие на сервере файлы. Нормально.
 
## Запуск краулеров

Запускаем краулеры по порядку (одновременно краулеры вроде бы не придумали, как запускать пока). Для запуска:

1. Спускаемся **до верхней папки "thai_scrapy"**. Проверяем, что там есть пустая папка для склада туда страниц. 

2. Пишем **"nohup scrapy crawl `<name>`** и запускаем (`<name>` - это то, что у Вас написано в строке **"name = '`<name>`'"** файла "thai_spyder.py").

3. Чтобы краулер продолжал краулить и после отключения от сервера, нажмите **ctrl + C**.

## Остановка краулеров

1. Для того, чтобы понять, когда можно останавливать, Вы должны проверять количество скаченных страниц. Ваш друг - команда **"find . -type | wc -l"**. Это команда высчитывает, сколько файлов внутри папки ( в том числе внутри папок папки и т.д.). Вы проверяете этой командой количество файлов в папке через какие-то произвольные промежутки времени и, когда число перестает расти, можете убивать программу -  все страницы скачены.

2. Для того, чтобы убить процесс, Вы набираете **"ps"** и находите  в полученных результатах ID нужного процесса (в самом левом столбике).

3. Дальше набираете **"kill `<process id>`"**. Ваш процесс убит. Можете запускать следующий.

## Конец

В какой-то момент надо остановится. После хорошо проделанной работы купите себе **молочный коктейль** (или что-нибудь другое, если Вы не переносите лактозу, не любите молочные коктейли, на диете) . Вы заслужили.



